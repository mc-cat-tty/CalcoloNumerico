Sono metodi che utilizzano una formulazione ricorsiva per generare una successione $\{x^{(k)}\}_{k \in \mathbb{N}} \rightarrow x^*$ per $k \leftarrow +\infty$, dove $x^*$ è la soluzione del sistema.

# Definizioni
## Convergenza per una successione di vettori
> La successione $\{x^{(k)}\}_{k \in \mathbb{N}} \subset \mathbb{R}^n$ converge al punto $x^*$ se $lim_{x \rightarrow \infty}{\|x^{(k)} - x^*\|} = 0$ per qualche norma $\|\cdot\|$
> Questo avviene se e solo se $lim_{x \rightarrow \infty}{\|x^{(k)}\|} = x^*$

### Esempio in $\mathbb{R}^1$
$x \in \mathbb{R}^1$
$$lim_{k \rightarrow \infty} \frac{k+1}{k} = 1 = x^* \Leftrightarrow lim_{k \rightarrow \infty} | \frac{k+1}{k} - 1 | = 0$$
$$\forall\epsilon\gt0 \space \exists \bar k: \forall k \gt \bar k |\frac{k+1}{k}-1| \lt \epsilon$$

Sulla retta dei numeri reali la successione si avvicina da destra a 1

### Esempio in $\mathbb{R}^2$
Sulla retta dei numeri reali è facile calcolare la distanza tra due punti, dato che si usa una semplice differenza messa in modulo.

In $n$ dimensioni vogliamo uno scalare che racchiuda l'informazione di lontananza tra i due punti. La norma 2 vettoriale è proprio una distanza euclidea estesa ad n dimensioni.

La norma sposta il problema in $\mathbb{R}^1$, dato che porta qualsiasi vettore in $\mathbb{R}^n$ in $\mathbb{R}^1$: $\|\cdot\|: \mathbb{R}^n \rightarrow \mathbb{R}^1$

In due dimensioni la distanza è come se venisse misurata "col righello": $\sqrt{(x_1^{(k)}-x_1^*)^2 + (x_2^{(k)}-x_2^*)^2}$

La formula usata nei metodi iterativi permette di calcolare $x^{(k+1)}$ a partire da $x^{(k)}$

#Attenzione: il prodotto vettoriale non gode della proprietà commutativa. La moltiplicazione a destra o a sinistra di matrici è differente.

# Riscrittura sistema per soluzione iterativa
1. $Ax = b$
2. $Ax - Ax = b - Ax$
3. $0 = b - Ax$
4. $Mx = Mx + b - Ax$ con M non singolare (deve esistere l'inversa)
5. $Mx = (M-A)x + b$
6. $x = M^{-1} (M-A)x + M^{-1} b$
7. $x = (I -  M^{-1}A) x + M^{-1} b$
8. $x = Gx + c$ chiamando $(I -  M^{-1}A) = G \in \mathbb{R}^{n \times n}$ e $M^{-1}b = c \in \mathbb{R}^{n \times 1}$

Trovare $Ax^* = b$ è equivalente a trovare $x^{(k+1)} = Gx^{(k)} + c$
In modo equivalente si può dire che $x$ resta uguale per due iterate successive

## Quesiti
- Come scegliere M?
- Quanto costa invertirla?

## Nomenclatura
- G - matrice di iterazione
- M - matrice del metodo -> la sua scelta porta ad una convergenza più o meno lenta della soluzione
- $x^{(0)} \in \mathbb{R}^n$ - punto iniziale -> se M è scelto correttamente, si arriva a convergenza indipendentemente dal punto iniziale

Il metodo si dice convergente se $\forall x^{(0)} \in \mathbb{R}$ la successione generata converge e a $x^*$.

#Nota: questo non può che essere un ragionamento astratto, dato che $x^*$ non è conosciuto a priori

# Condizioni di terminazione
Idealmente: dato un errore $\epsilon$ si vuole determinare quale $k$ soddisfa $\|x^{(k)} - x^*\| \lt \epsilon$

## Distanza relativa
Si usa l'errore relativo tra due iterazioni: $\frac{\|x^{(k+1)} - x^{(k)}\|}{\|x^{(k+1)}\|} \lt \tau$
Controllare la distanza tra due iterazioni successive è equivalente a controllare la distanza della soluzione corrente da $x^*$, che però non conosciamo.

## Residuo
Un metodo alternativo consiste nel calcolare il residuo al passo k: $r^{(k)} = b - Ax^{(k)}$
Il residuo diventa nullo se $x^{(k)}$ è la soluzione del sistema.

Anche in questo caso si definisce una tolleranza tau $\tau$. L'arresto avviene nel caso in cui $\frac{\|r^{(k)}\|}{\|b\|} \lt \tau$
Il residuo viene normalizzato su $b$ perché è più o meno sullo stesso ordine di grandezza di $A$ (stessa scala).

## Parametro di salvaguardia
In aggiunta ad uno dei primi due criteri, si aggiunge un **parametro di salvaguardia**, dato che i primi due criteri possono non portare alla terminazione dell'algoritmo nel caso in cui il sistema sia *mal condizionato*.

Il parametro di salvaguardia definisce un numero massimo di iterazioni dopo le quali si interrompe l'iterazione. È un criterio meramente pratico

## Dimostrazione condizione sufficiente per la convergenza
### Tesi
> Se $\|G\| \lt 1 \Rightarrow x^{(k+1)} = G x^{(k)} + c$ con $k \in \mathbb{N}$ è convergente

Ricordando proposizioni sulle norme (sempre positiva, lineare rispetto ai coefficienti, conforme alla disuguaglianza triangolare, proprietà submoltiplicativa - $\|AB\| \leq \|A\|\|B\|$, compatibilità con le norme vettoriali) si ha che: $0 \lt \|G\| \lt 1$

#Idea Una matrice di iterazione che rispetta questa condizione amplifica "poco" il vettore x, quindi il miglioramento della successione verso $x^*$ sarà molto piccolo.

Un'utile riscrittura della tesi è: $$ se \space \|G\| \lt 1 \Rightarrow lim_{k \rightarrow \infty} x^{(k)} = x^*$$

La condizione è sufficiente, ma non necessaria, perché esistono dei sistemi per cui si arriva a convergenza anche con $\|G\| \geq 1$

### Dimostrazione
Si definisce $e^{(k)} := x^{(k)} - x^*$

Rielaborando la tesi:
$$lim_{k \rightarrow \infty} x^{(k)} = x^* \Leftrightarrow lim_{k \rightarrow \infty} x^{(k)} - x^* = 0 \Leftrightarrow lim_{k \rightarrow \infty} e^{(k)}= 0$$

Sviluppando $e^{(k)}$:
- $e^{(k)} := x^{(k)} - x^*$
- $x^{(k)} - x^* = Gx^{(k-1)} + c - G x^* - c$ dalla definizione "ricorsiva" di $x^{(k)}$
- $Gx^{(k-1)} + c - G x^* - c = Gx^{(k-1)} - G x^*$
- $Gx^{(k-1)} - G x^* = G (x^{(k-1)} - x^*)$
- $G (x^{(k-1)} - x^*) = G e^{(k-1)}$

Quindi $e^{(k)}$ può essere definito ricorsivamente: $e^{(k)} = G e^{(k-1)}  = G^2 e^{(k-2)} = \dots = G^k e^{(0)}$

$$lim_{k \rightarrow \infty} e^{(k)} = lim_{k \rightarrow \infty} G^k e^{(0)}$$

- dalla submoltiplicatività $\|G^k e^{(0)}\| \leq \|G^k\| \|e^{(0)}\|$
- dalla positività $\|G^k e^{(0)}\| \geq 0$
- applicando $k$ volte la submoltiplicatività $\|G^k\| = \|G \boldsymbol{\cdot} G  \boldsymbol{\cdot} \dots  \boldsymbol{\cdot} G\| = \|G^{(k-1)}G\| \leq \|G^{k-1}\|\|G\|=\|G^{k-2}\|\|G\|^2 \leq \|G^{k-2}\|\|G\|^2 \leq \dots \leq \|G\|^k$ per $k \gt 0$

Inoltre se $\|G\| \lt 1 \Leftrightarrow \|G\| = \frac{1}{p}$ con $p \gt 1$. Per $p \rightarrow \infty$ tende a 0

Ne deriva che il limite dell'errore è schiacciato nel seguente modo:
$$0 \leq lim_{k \rightarrow \infty} \|G^k e^{(0)}\| \leq lim_{k \rightarrow \infty} \|G^k\| \|e^{(0)}\| \leq lim_{k \rightarrow \infty} \|G\|^k \|e^{(0)}\| \leq \|e^{(0)}\| lim_{k \rightarrow \infty} \|G\|^k = 0$$

Dal teorema del confronto (carabinieri): $lim_{k \rightarrow \infty} \|G^k e^{(0)}\| = 0 \Leftrightarrow lim_{k \rightarrow \infty} x^{(k)} = x^*$

## Dimostrazione condizione necessaria e sufficiente per la convergenza
### Tesi 
> Un metodo iterativo è convergente se e solo se $\rho(G) \lt 1$

Il raggio spettrale di una matrice è il massimo dei suoi autovalori, in modulo:
$$\rho(G) = max_{i \in [1, n]}|\lambda_i|$$
con $\lambda_i \space \forall i \in [1, n]$ autovalore di $G$

### Dimostrazione
Si può dimostrare che $lim_{k \rightarrow \infty} \|G\|^k = 0 \Leftrightarrow \rho(G) \lt 1$

Inserendo questa considerazione nella dimostrazione precedente, non è necessario supporre $\|G\| \lt 1$

### Velocità di convergenza
La velocità di convergenza è inversamente proporzionale a $\rho(G)$. Tanto più il raggio spettrale è piccolo, tanto più velocemente il metodo iterativo convergerà.

## Stima dell'errore assoluto
Obiettivo: fissato un certo $\epsilon$ - soglia dell'errore tra la x corrente e la soluzione del sistema - posso calcolare un $\tau$ differenza tra due iterate successive per cui la condizione sull'errore $\epsilon$ è soddisfatta.

Hp:
- $\tau \gt 0$
- per un certo $k$ si ha che $\|x^{(k+1)} - x^{(k)}\| \lt \tau$

Th: $\|x^{(k)} - x^*\| \leq \epsilon$ con $\epsilon = \tau \|(G-I)^{-1}\|$.

Si dimostra che se $(G-I)$ è non singolare, la successione converge.

Data $x^*$ soluzione del sistema: $x^* = Gx^* + c$

### Dimostrazione
$$x^{(k+1)} - x^{(k)} = Gx^{(k)}+c-x^{(k)} = Gx^{(k)}-Gx^*+x^*-x^{(k)} = G(x^{(k)}-x^*)-I(x^{(k)}-x^*)=(G-I)(x^{(k)}-x^*)$$

Si dimostra che $(G-I)$ è invertibile: $G-I = I-M^{-1}A-I = -M^{-1}A$. Nel primo passaggio si usa la definizione di $G$. Dal teorema di Binet la matrice di sinistra è invertibile.

Potendo invertire $(G-I)$  possibile scrivere $(G-I)^{-1}(x^{(k+1)} - x^{(k)}) = x^{(k)} - x^*$

Usando la submoltiplicatività della norma: $\|x^{(k)} - x^*\| = \|(G-I)^{-1}(x^{(k+1)} - x^{(k)})\| \leq \|(G-I)^{-1}\| \|x^{(k+1)} - x^{(k)}\|$

## Stima dell'errore relativo
Supponendo che $x^*$ e $x^{(k+1)}$ abbiano all'incirca lo stesso ordine di grandezza, l'errore relativo è: $$\frac{\|x^{(k)} - x^*\|}{\|x^*\|} \approx \frac{\|x^{(k)} - x^*\|}{\|x^{(k+1)}\|} \leq \|(G-I)^{-1}\| \frac{\|x^{(k+1)} - x^{(k)}\|}{\|x^{(k+1)}\|}$$

Solitamente al secondo membro si usa $\tau$ poiché il coefficiente $\|(G-I)^{-1}\|$ non è conosciuto

## Proprietà sul residuo del sistema
Hp:
- Fissato $\tau \gt 0$
- se $\frac{\|r^{(k)}\|}{\|b\|} \leq \tau$; si usa $b$ perché ha circa lo stesso ordine di grandezza di $A$ e $x$ (non è certo, ma operativamente è molto probabile)

Th: $\frac{\|x^* - x^{(k)}\|}{\|x^*\|} \leq k(A)\tau$

$k(A)$ è il numero di condizionamento di $A$. La matrice è ben condizionata se questo numero è piccolo. Nel LHS della frazione si trova una possibile formulazione dell'errore relativo.

> Se il sistema è mal condizionato soddisfare una tolleranza bassa potrebbe essere proibitivo

## Recap criteri di arresto
1. $\frac{\|x^{(k+1)}-x^{(k)}\|}{\|x^{(k+1)}\|} \lt \tau$ vicinanza alla soluzione
2. $\frac{\|r^{(k)}\|}{\|b\|} \lt \tau$ buon condizionamento

# Pseudocodice
```
init x = x0  // con x0 casuale

for kMax iterations { // unless safeguard param is triggered
	r = A * x0 - b  // compute residual 
	x = G * x0 + c  // update x
	if (norm(x - x0) < tau * norm(x)) and (norm(r) < tau * norm(b)) {
		return x
	}
	x0 = x
}

throw ill-conditioned system error
```

#Nota: nella realtà si usano metodi misti, iterativi per N iterazioni, poi si applicano metodi diretti

# Scelta di M
Ricordando:
- $G = I - M^{-1}A$
- $c = M^{-1} b$

La scelta della matrice di metodo è un trade-off. Con $M = A$ si arriverebbe alla soluzione in 1 sola iterazione. Questa scelta implica l'inversione (costosissima) di $A$.

Una buona matrice di metodo deve avere le seguenti caratteristiche:
1. essere simile ad A: $M \sim A$ -> $M^{-1}A \sim I$ -> $G = (M^{-1}A - I) \sim 0$. In questo modo $\rho(G)$ sarebbe piccolo e porterebbe ad una convergenza veloce.
2. struttura "semplice": diagonale, triangolare superiore o inferiore -> la risoluzione di $Mx^{(k+1)} = \Sigma^{(k)}$, con $\Sigma^{(k)} = (M-A)x^{(k)} + b$, è semplice se anche la matrice M lo è

# Metodi di decomposizione
Si basano sulla decomposizione di A in M ed N: $A = M - N \leftrightarrow M = A + N$

Questa decomposizione di A viene usata dai metodi di Jacobi e Gauss Seidel.

$$A = M - N = - \begin{pmatrix} 0 & 0 & \dots & 0 \\ -a_{21} & 0 & \dots & 0 \\\vdots & \vdots & \ddots & \vdots \\ -a_{n1} & -a_{n2} & \dots & 0 \\ \end{pmatrix} + \begin{pmatrix} a_{11} & 0 & \dots & 0 \\ 0 & a_{22} & \dots & 0 \\\vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & a_{nn}\\ \end{pmatrix} - \begin{pmatrix} 0 & -a_{12} & \dots & -a_{1n} \\ 0 & 0 & \dots & -a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 0 \\ \end{pmatrix}$$

Riarrangiando l'ordine delle matrici: $$A = D - E - F = \begin{pmatrix} a_{11} & 0 & \dots & 0 \\ 0 & a_{22} & \dots & 0 \\\vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & a_{nn}\\ \end{pmatrix} - \begin{pmatrix} 0 & 0 & \dots & 0 \\ -a_{21} & 0 & \dots & 0 \\\vdots & \vdots & \ddots & \vdots \\ -a_{n1} & -a_{n2} & \dots & 0 \\ \end{pmatrix} - \begin{pmatrix} 0 & -a_{12} & \dots & -a_{1n} \\ 0 & 0 & \dots & -a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 0 \\ \end{pmatrix}$$
In base alla ripartizione di D, E ed F otteniamo due metodi iterativi:
- **Metodo di Jacobi**: $M = D$, $N = E + F$
- **Metodo di Gauss-Sidel**: $M = D - E$, $N = F$

Nei metodi iterativi l'obiettivo è ottenere $Ax = b$ -> $Mx - Nx = b$ -> $Mx = Nx + b$
In forma "ricorsiva": $Mx^{(k+1)} = Nx^{(k)} + b$
Nella riscrittura $Mx^{(k+1)} = p$ con $p = Nx^{(k)} + b$ basta risolvere il sistema $Mx^{(k+1)} = p$ per trovare il valore di $x$ per l'iterata successiva.

## Metodo di Jacobi
Chiamato anche metodo degli spostamenti **simultanei** -> parallelizzabile.
- $M = D$ diagonale
- $N = E + F$

$$Dx^{(k+1)} = (E+F)x^{(k)} + b$$
L'inverso della matrice $D$ è facilmente calcolabile sostituendo ogni elemento sulla diagonale con il suo reciproco. Tutte le divisioni possono essere eseguite simultaneamente.

In Matlab $D$ può essere facilmente estratta con `diag(diag(A))`:
```Matlab
d = diag(A);
p = (E + F) x + b;
x_next = d .\ p;
```

## Metodo di Gauss-Sidel
Chiamato anche metodo degli spostamenti **successivi** -> non parallelizzabile.
- $M = D - E$ matrice triangolare inferiore
- $N = F$ matrice triangolare superiore

 $$(D-E)x^{(k+1)} = Fx^{(k)} + b$$
Ad ogni iterazione serve conoscere il risultato dell'equazione precedente per poter procedere, dato che al LHS si trova una matrice triangolare inferiore -> **sostituzione in avanti** (metodo degli spostamenti successivi)

## Teorema di convergenza dei metodi di Jacobi e di Gauss-Seidel
> Le matrici a diagonale dominante ($|a_{ii}| \gt \sum_{j=1, j \neq i}^n |a_{ij}| \space \forall i \in [1, n]$) portano i metodi di Jacobi e Gauss-Seidel alla convergenza.

#Nota: le matrici a diagonale dominante sono innanzitutto invertibili

#Nota: la valutazione dei criteri di arresto deve essere fatta con una norma adeguata, conforme a $\| \cdot \|_\infty$, come ad esempio $\| \cdot \|_2$

### Dimostrazione per Jacobi
Chiamiamo:
- $J = D^{-1}$ (E+F)
- $c = D^{-1}b$

Hp: $A$ a diagonale dominante

Th: convergenza -> $\|J\|_\infty \lt 1$

Dimostriamo che $\|J\|_\infty \lt 1$ ricordando che la norma infinito di una matrice è il massimo valore tra le sommatorie per righe della matrice ($\| \cdot \|_\infty = max \sum_{j=1, j=i}^n |a_{ij}| \space \forall i \in [1, n]$).

La matrice $J$ è costituita nel modo seguente: $$\begin{pmatrix} 0 & -\frac{a_{12}}{a_{11}} & \dots & -\frac{a_{1n}}{a_{11}} \\ -\frac{a_{21}}{a_{22}} & 0 & \dots & -\frac{a_{2n}}{a_{22}} \\ \vdots & \vdots & \ddots & \vdots \\ -\frac{a_{n1}}{a_{nn}} & -\frac{a_{n2}}{a_{nn}} & \dots & 0 \\ \end{pmatrix}$$
La norma infinito di $J$ diventa: $\| J \|_\infty = max \frac{1}{|a_{ii}|} \sum_{j=1, j=i}^n |a_{ij}| \space \forall i \in [1, n]$

Dall'ipotesi:
- $|a_{ii}| \gt \sum_{j=1, j \neq i}^n |a_{ij}| \space \forall i \in [1, n]$
- $\frac{|a_{ii}|}{a_{ii}} \gt \frac{1}{|a_{ii}|} \sum_{j=1, j \neq i}^n |a_{ij}| \space \forall i \in [1, n]$
- $1 \gt \frac{1}{|a_{ii}|} \sum_{j=1, j \neq i}^n |a_{ij}| \space \forall i \in [1, n]$
- $1 \gt \|J\|_\infty$

### Dimostrazione per Gauss-Seidel
#TODO 
